1. The code incorrectly corrects the word "alright" to "all_right"
In the aspell.txt the all_right version is positioned as the correct one, 
and alright as the typo, even though it is perfectly accetable in English.
The algorythm still corrects it to all_right because it blindly follows the dataset.

2. The code incorreectly "corrects" the word acommodate into acommodate. 
The typo acommodate isn’t listed in aspell.txt. The model only fixes the typos it has seen before,
and the HMM only models letter substitutions, not insertions or deletions. Since acommodate
is missing a "c", the algorithm can’t recognize or repair it, so it just leaves it unchange even though it’s still incorrect.

3. The code correctly corrects nevade into Nevada. This typo appears directly in the aspell.txt.
The code detects that and proceeds to output the capitalized version of nevada.
Because this pair exists in the training data, the correction is exact and confident.

4.
Real typos (from real users) capture how people actually make mistakes fe. missing doubles, nearby keyboard hits, phonetic confusions.
So the model would learn realistic (real world) emission probabilities.

Synthetic typos (generated by code) don’t reflect real human errors. They’re random or code-genereted, not natural.
So the model learns unrealistic patterns.